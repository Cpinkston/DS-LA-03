{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This is a walkthrough of [Clustering text documents using k-means](http://scikit-learn.org/stable/auto_examples/document_clustering.html#example-document-clustering-py) from Scikit-Learn.\n",
      "\n",
      "This is an example showing how the scikit-learn can be used to cluster documents by topics using a bag-of-words approach. \n",
      "\n",
      "> Do we remember what bag-of-words is?\n",
      "\n",
      "This example uses a scipy.sparse matrix to store the features instead of standard numpy arrays.\n",
      "\n",
      "> What is scipy.sparse matrix?\n",
      "\n",
      "Two feature extraction methods can be used in this example:\n",
      "\n",
      "- TfidfVectorizer uses a in-memory vocabulary (a python dict) to map the most frequent words to features indices and hence compute a word occurrence frequency (sparse) matrix. \n",
      "\n",
      "> What is Tfidf ?\n",
      "> What is *word occurrence frequency* ?\n",
      "\n",
      "- The word frequencies are then reweighted using the Inverse Document Frequency (IDF) vector collected feature-wise over the corpus.\n",
      "\n",
      "- HashingVectorizer hashes word occurrences to a **fixed dimensional space**, possibly with **collisions**. The word count vectors (Term Frequencies or Counts) are then **normalized** to each have **l2-norm** equal to one (projected to the euclidean unit-ball) which seems to be important for **k-means to work in high dimensional space.**\n",
      "\n",
      "> The 'trick' that HashingVectorizer performs is to Hash the words (mapping all possible feature values (in this case, unigrams... or single words) to a fixed set of values. 'Collisions' means that some words will possibly map to the same value."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import HashingVectorizer\n",
      "hv = HashingVectorizer(n_features=3, norm='l1', non_negative=False)\n",
      "corpus = [\n",
      "'This is the first document.',\n",
      "'This is the second second document.',\n",
      "'And the third one.',\n",
      "'Is this the first document?',\n",
      "]\n",
      "hashed_corpus = hv.transform(corpus)\n",
      "print hashed_corpus"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  (0, 0)\t0.0\n",
        "  (0, 1)\t0.333333333333\n",
        "  (0, 2)\t-0.666666666667\n",
        "  (1, 0)\t0.0\n",
        "  (1, 1)\t0.75\n",
        "  (1, 2)\t-0.25\n",
        "  (2, 0)\t0.0\n",
        "  (2, 2)\t-1.0\n",
        "  (3, 0)\t0.0\n",
        "  (3, 1)\t0.333333333333\n",
        "  (3, 2)\t-0.666666666667\n"
       ]
      }
     ],
     "prompt_number": 62
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- HashingVectorizer does not provide IDF weighting as this is a stateless model (the fit method does nothing)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "hv.fit(corpus)  # This does nothing."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "When IDF weighting is needed it can be added by pipelining its output to a TfidfTransformer instance."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "> As a preview, this is what they mean by **pipelining**\n",
      "\n",
      "```python\n",
      "vectorizer = Pipeline((\n",
      "     ('hasher', HashingVectorizer(n_features= ...),\n",
      "     ('tf_idf', TfidfTransformer())\n",
      " ))\n",
      "\n",
      "X = vectorizer.fit_transform(dataset.data)\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Two algorithms are demoed: ordinary k-means and its more scalable cousin minibatch k-means.\n",
      "\n",
      "> [Minibatch k-means](http://scikit-learn.org/stable/modules/clustering.html#mini-batch-kmeans) iteratively uses random samples of the data to do k-means.\n",
      "\n",
      "It can be noted that k-means (and minibatch k-means) are very sensitive to **feature scaling** and that in this case the IDF weighting helps improve the quality of the clustering by quite a lot as measured against the \u201cground truth\u201d provided by the class label assignments of the 20 newsgroups dataset.\n",
      "\n",
      "> What is does \"sensitive to **feature scaling**\" mean?\n",
      "\n",
      "This improvement is not visible in the Silhouette Coefficient which is small for both as this measure seem to suffer from the phenomenon called \u201cConcentration of Measure\u201d or \u201cCurse of Dimensionality\u201d for high dimensional datasets such as text data.\n",
      "\n",
      "> What is \u201cCurse of Dimensionality\u201d  ?\n",
      "\n",
      "Other measures such as V-measure and Adjusted Rand Index are information theoretic based evaluation scores: as they are only based on cluster assignments rather than distances, hence not affected by the curse of dimensionality.\n",
      "\n",
      "> [V-measure](http://scikit-learn.org/stable/modules/clustering.html#homogeneity-completeness-and-v-measure) is the harmonic mean of homogeneity and completeness. More info found in the link.\n",
      "> [Adjust Rand Index](http://scikit-learn.org/stable/modules/clustering.html#adjusted-rand-index) is a measure of the similarity of sets of assignments. In this case, it measures how the predicted clustering is to the true clustering. \n",
      "\n",
      "Note: as k-means is optimizing a non-convex objective function, it will likely end up in a local optimum. Several runs with independent random init might be necessary to get a good convergence.\n",
      "\n",
      "> What does **non-convex** mean?\n",
      "\n",
      "> What's the objective function for k-means?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    }
   ],
   "metadata": {}
  }
 ]
}